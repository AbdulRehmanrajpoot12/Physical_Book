"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[3267],{454(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview of Vision-Language-Action pipelines integrating LLMs with robotics for perception, planning, and action","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/Physical_Book/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/AbdulRehmanrajpoot12/Physical_Book/edit/main/docs/module-4/index.md","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"title":"Module 4: Vision-Language-Action (VLA)","description":"Overview of Vision-Language-Action pipelines integrating LLMs with robotics for perception, planning, and action"},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Humanoid Navigation","permalink":"/Physical_Book/docs/module-3/nav2-humanoid-navigation"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/Physical_Book/docs/module-4/llm-planning"}}');var t=i(4848),s=i(8453);const a={sidebar_position:0,title:"Module 4: Vision-Language-Action (VLA)",description:"Overview of Vision-Language-Action pipelines integrating LLMs with robotics for perception, planning, and action"},l="Module 4: Vision-Language-Action (VLA)",r={},c=[{value:"Module Overview",id:"module-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Getting Started",id:"getting-started",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"Welcome to Module 4 of the Physical AI & Humanoid Robotics book. This module focuses on Vision-Language-Action (VLA) pipelines that integrate LLMs with robotics for perception, planning, and action. The module builds upon the ROS 2 fundamentals, simulation concepts, and NVIDIA Isaac technologies from previous modules to create complete autonomous humanoid systems."}),"\n",(0,t.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,t.jsx)(n.p,{children:"This module covers three critical aspects of autonomous humanoid systems:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./voice-to-action",children:"Voice-to-Action"})})," - Explore speech recognition using OpenAI Whisper and converting voice commands to robot intents. Learn how to create natural voice interfaces for humanoid robots."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./llm-planning",children:"Cognitive Planning with LLMs"})})," - Discover how to translate natural language into action sequences and map plans to ROS 2 actions. Understand how LLMs can enhance robotic planning capabilities."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./autonomous-humanoid",children:"Capstone \u2013 The Autonomous Humanoid"})})," - Master end-to-end system integration with perception, navigation, manipulation pipelines. Build complete AI-powered humanoid robots that integrate all VLA components."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By completing this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Process voice commands using OpenAI Whisper and convert them to robot intents"}),"\n",(0,t.jsx)(n.li,{children:"Use LLMs for cognitive planning and translating natural language to action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Map high-level plans to specific ROS 2 actions for robot execution"}),"\n",(0,t.jsx)(n.li,{children:"Integrate perception, navigation, and manipulation in complete autonomous systems"}),"\n",(0,t.jsx)(n.li,{children:"Implement Vision-Language-Action pipelines for natural human-robot interaction"}),"\n",(0,t.jsx)(n.li,{children:"Design end-to-end autonomous humanoid systems with safety and reliability"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Understanding of ROS 2 fundamentals from ",(0,t.jsx)(n.a,{href:"../module-1/ros2-basics",children:"Module 1: The Robotic Nervous System (ROS 2)"})]}),"\n",(0,t.jsxs)(n.li,{children:["Knowledge of simulation concepts from ",(0,t.jsx)(n.a,{href:"../module-2/digital-twins-physical-ai",children:"Module 2: The Digital Twin (Gazebo & Unity)"})]}),"\n",(0,t.jsxs)(n.li,{children:["Experience with NVIDIA Isaac technologies from ",(0,t.jsx)(n.a,{href:"../module-3/nvidia-isaac-sim",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"})]}),"\n",(0,t.jsx)(n.li,{children:"Basic understanding of LLMs and their applications in robotics"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsxs)(n.p,{children:["Begin with the ",(0,t.jsx)(n.a,{href:"./voice-to-action",children:"Voice-to-Action"})," chapter to understand the foundational voice processing capabilities that enable natural human-robot interaction in this module."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);